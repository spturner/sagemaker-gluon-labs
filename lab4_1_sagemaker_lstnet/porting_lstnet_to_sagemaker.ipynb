{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Porting LSTNet to Amazon SageMaker\n",
    "\n",
    "In this lab the LSTNet model is ported to be trained using Amazon Sagemaker.\n",
    "\n",
    "It starts will assuming that an LSTNet model has already been developed. In the same directory as this notebook there are several modules containing supporting classes:\n",
    "\n",
    "1. lstnet.py - The declaration of the model and forward function. The model consists of a convolutional layer, dropout, a gru, a skip gru, a fully connected layer, and the parallel autoregresive component.\n",
    "2. timeseriesdataset.py - Classes for loading the data. TimeSeriesData, and TimeSeriesDataset. TimeSeriesDataset is a subclass of gluon.data.Dataset. It implements the __getitem__ function which returns a time series for the given index. These classes are used to load an input file and to generate successive examples with a specified window and horizon. The window is the length of timeseries used as input data for the prediction and the horizon is the number of time steps between the end of the window and the time at which the prediction is for.\n",
    "2. lstnet_sagemaker.py - This module implements the train() function which is used as the entrypoint for training the model on a server. This is called by Amazon SageMaker on each host in the training cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Variables\n",
    "\n",
    "Configure the following variables for your environment:\n",
    "\n",
    "1. bucket - The bucket name to be used to store the training data and model artifacts.\n",
    "2. prefix - The folder name which is used inside the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'eduthie-sagemaker-1'\n",
    "prefix = 'lstnet'\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "data_dir = '../../data'\n",
    "data_file_path = os.path.join(data_dir,'electricity.txt')\n",
    "\n",
    "test_bucket_prefix = '/test/'\n",
    "single_host_train_bucket_prefix = '/train/single_host/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data\n",
    "\n",
    "The first step is to load the electricity dataset from a file. The dataset itself is included in this github repo in the data directory. \n",
    "\n",
    "* The data is normalised so each reading is between 0 and 1. This is done by dividing each column by the maximum value of the column. A column is an electricity consumption time series for a single customer.\n",
    "\n",
    "There are 321 time series of electricity consumption with 26,304 time periods in each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26304, 321)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_file_path,header=None)\n",
    "max_columns = df.max().astype(np.float64)\n",
    "df = df/max_columns # normalize\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Training and Test and Upload to S3\n",
    "\n",
    "The first 80% of the time series is used for training and the last 20% is used as a test set.\n",
    "\n",
    "These datasets are written to a csv file and then uploaded to S3 to be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size 21043\n",
      "Test size 5261\n"
     ]
    }
   ],
   "source": [
    "train_frac = 0.8\n",
    "\n",
    "num_time_steps = len(df)\n",
    "split_index = int(num_time_steps*train_frac)\n",
    "train = df[0:split_index]\n",
    "print('Training size {}'.format(len(train)))\n",
    "test = df[split_index:]\n",
    "print('Test size {}'.format(len(test)))\n",
    "\n",
    "test_file_path = os.path.join(data_dir,'test.csv')\n",
    "test.to_csv(test_file_path,header=None,index=False)\n",
    "train_file_path = os.path.join(data_dir,'train.csv')\n",
    "train.to_csv(train_file_path,header=None,index=False)\n",
    "\n",
    "client = boto3.client('s3')\n",
    "client.upload_file(test_file_path, bucket, prefix + test_bucket_prefix + 'test.csv')\n",
    "client.upload_file(train_file_path, bucket, prefix + single_host_train_bucket_prefix + 'train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Locally\n",
    "\n",
    "To make sure there are no obvious bugs in the code, the train() function is called in the notebook. This is done with 1 epoch to verify that it executed correctly. There are also some basic unit tests included in the directory. A notebook instance with a GPU is requred to execute the following steps. Otherwise skip on to deploying with SageMaker.\n",
    "\n",
    "The key parameters to the train() function in this case are:\n",
    "\n",
    "        - hyperparameters: The Amazon SageMaker Hyperparameters dictionary. A dict\n",
    "            of string to string.\n",
    "        - channel_input_dirs: A dict of string-to-string maps from the\n",
    "            Amazon SageMaker algorithm input channel name to the directory containing\n",
    "            files for that input channel. Note, if the Amazon SageMaker training job\n",
    "            is run in PIPE mode, this dictionary will be empty.\n",
    "        - output_data_dir:\n",
    "            The Amazon SageMaker output data directory. After the function returns, data written to this\n",
    "            directory is made available in the Amazon SageMaker training job\n",
    "            output location.\n",
    "        - num_gpus: The number of GPU devices available on the host this script\n",
    "            is being executed on.\n",
    "        - num_cpus: The number of CPU devices available on the host this script\n",
    "            is being executed on.\n",
    "        - hosts: A list of hostnames in the Amazon SageMaker training job cluster.\n",
    "        - current_host: This host's name. It will exist in the hosts list.\n",
    "        - kwargs: Other keyword args."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file path ../../data/test.csv\n",
      "Test file path ../../data/test.csv\n",
      "Loading file ../../data/test.csv\n",
      "Loading file ../../data/test.csv\n",
      "Is it a file True\n",
      "Data length 5261\n",
      "Loading file ../../data/test.csv\n",
      "Loading file ../../data/test.csv\n",
      "Is it a file True\n",
      "Data length 5261\n",
      "Running on [gpu(0)]\n",
      "Hosts ['alg-1']\n",
      "Current Host alg-1\n",
      "kvstore device\n",
      "Training Start\n",
      "Epoch 0: rmse 0.31743179211536277 time 4.5512 s\n",
      "Final rmse 0.23415213010202235\n",
      "Total training time: 8.440000057220459\n",
      "Training End\n"
     ]
    }
   ],
   "source": [
    "from lstnet_sagemaker import train\n",
    "hyperparameters = {\n",
    "    'conv_hid' : 10,\n",
    "    'gru_hid' : 10,\n",
    "    'skip_gru_hid' : 2,\n",
    "    'skip' : 5,\n",
    "    'ar_window' : 6,\n",
    "    'window' : 24*7,\n",
    "    'horizon' : 24,\n",
    "    'learning_rate' : 0.01,\n",
    "    'clip_gradient' : 10.,\n",
    "    'batch_size' : 128,\n",
    "    'epochs' : 1\n",
    "}\n",
    "channel_input_dirs = {'train':data_dir,'test':data_dir}\n",
    "train(\n",
    "    hyperparameters = hyperparameters,\n",
    "    input_data_config = None,\n",
    "    channel_input_dirs = channel_input_dirs,\n",
    "    output_data_dir = os.path.join(data_dir,'output'),\n",
    "    model_dir = None,\n",
    "    num_gpus = 1,\n",
    "    num_cpus = 1,\n",
    "    hosts = ['alg-1'],\n",
    "    current_host = 'alg-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose Hyperparameters\n",
    "\n",
    "In one of the next sessions the automatic model tuning using Bayesian Optimisation will be covered. In this case a set of reasonable hyperparameters are chosen. Can you tweak these to make the network converge faster to a lower rmse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'conv_hid' : 100,\n",
    "    'gru_hid' : 100,\n",
    "    'skip_gru_hid' : 5,\n",
    "    'skip' : 24,\n",
    "    'ar_window' : 24,\n",
    "    'window' : 24*7,\n",
    "    'horizon' : 24,\n",
    "    'learning_rate' : 0.001,\n",
    "    'clip_gradient' : 10.,\n",
    "    'batch_size' : 64,\n",
    "    'epochs' : 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigger the training job using the SageMaker Python API.\n",
    "\n",
    "The final step is to trigger the training job using the high-level Python API. A lower-level API is also available for more detailed control of the parameters.\n",
    "\n",
    "First an estimator is created with sagemaker.mxnet.MXNet. The inputs are:\n",
    "\n",
    "* entry_point='lstnet_sagemaker.py' - The module used to run the training by calling the train() function\n",
    "* source_dir='.' - An optional directory containing code with is copied onto the SageMaker training hosts and made available to the training script.\n",
    "* role=role - The IAM role which is given to the training hosts giving them privileges such as access to the S3 bucket.\n",
    "* output_path='s3://{}/{}/output'.format(bucket, prefix) - The S3 bucket to store artifacts such as the model parameters.\n",
    "* train_instance_count=1 - The number of hosts used for training. Using a number > 1 will start a cluster. To take advantage of this the trianing data should be sharded. See the next lab.\n",
    "* train_instance_type='ml.p3.2xlarge' - The EC2 instance type to be used for training hosts. In this case the latest generation p3 is chosen with a Nvidia Tesla v100 GPU.\n",
    "* hyperparameters=hyperparameters - The hyperparameter dictionary made available to the train() function in the endpoint script.\n",
    "\n",
    "Then the fit() method of the estimator is called. The parameters are:\n",
    "\n",
    "* inputs - A dictionary containing the URLs in S3 of the 'train' data directory and the 'test' data directory.\n",
    "* wait - This is specified as False so the fit() method returns immediately after the training job is created. Go to the SageMaker console to monitor the progress of the job. Set wait to True to block and see the progress of the training job output in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-2018-07-17-10-14-05-563\n"
     ]
    }
   ],
   "source": [
    "lstnet1 = MXNet(entry_point='lstnet_sagemaker.py',\n",
    "    source_dir='.',\n",
    "    role=role,\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    hyperparameters=hyperparameters)\n",
    "lstnet1.fit(inputs={'train': 's3://{}/{}{}'.format(bucket, prefix, single_host_train_bucket_prefix),\n",
    "    'test': 's3://{}/{}{}'.format(bucket, prefix, test_bucket_prefix)},wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations. You have successfully ported LSTNet to Amazon SageMaker. The next step is to modify it to run using multiple GPUs and multiple hosts to train faster.\n",
    "\n",
    "Next lab: [LSTNet Multi GPU and Distributed Training](../lstnet_multi_gpu_distributed/lstnet_multi_gpu_distributed.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
